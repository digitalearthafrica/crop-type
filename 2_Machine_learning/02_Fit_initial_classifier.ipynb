{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565e121-2001-4b0e-9789-f6eb1e15300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, balanced_accuracy_score, f1_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d80f6-4b05-48df-908e-572b0d468304",
   "metadata": {},
   "source": [
    "## Read in training data and label dictionary\n",
    "\n",
    "### Define the data and label paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1111f-41c0-456b-a175-304b6eb3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data file from previous step\n",
    "data_path = \"results/training_data_multipixel.txt\"\n",
    "\n",
    "# Dictionary with class labels from previous step\n",
    "labels_path = \"results/class_labels.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d203363-3d9e-48d9-90a7-8622a4302845",
   "metadata": {},
   "source": [
    "### Load the data and identify the feature columns for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda5dd0-f934-48c4-a711-0e25b642858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(data_path)\n",
    "\n",
    "# load the column_names\n",
    "with open(data_path, \"r\") as file:\n",
    "    header = file.readline()\n",
    "\n",
    "# Remove comment symbol from header, then extract label and feature names\n",
    "column_names = header.split()[1:]\n",
    "\n",
    "label_col = column_names[0]\n",
    "feature_cols = column_names[1:]\n",
    "\n",
    "print(f\"Label column:\\n{label_col}\\n\")\n",
    "print(f\"Feature columns:\\n{feature_cols}\\n\")\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]\n",
    "\n",
    "# Read the class label dictionary\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3188ad6-7c03-4856-b118-4731dbec8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef41850-c076-4251-b795-8c69905e3ded",
   "metadata": {},
   "source": [
    "## Configure settings for model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a41b1-196c-4832-bf93-5d3a04c1bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"exp_multipixel_allfeatures_removecorrfeaturesgt0p9\"\n",
    "\n",
    "# Decide which features to use\n",
    "use_all_features = True\n",
    "\n",
    "if use_all_features:\n",
    "    columns_to_use = feature_cols  # everything\n",
    "# Comment out any non-desired features\n",
    "else:\n",
    "    columns_to_use = [\n",
    "        \"blue_s2_Q3_2021\",\n",
    "        \"green_s2_Q3_2021\",\n",
    "        \"red_s2_Q3_2021\",\n",
    "        \"nir_s2_Q3_2021\",\n",
    "        \"swir_1_s2_Q3_2021\",\n",
    "        \"swir_2_s2_Q3_2021\",\n",
    "        \"red_edge_1_s2_Q3_2021\",\n",
    "        \"red_edge_2_s2_Q3_2021\",\n",
    "        \"red_edge_3_s2_Q3_2021\",\n",
    "        \"NDVI_s2_Q3_2021\",\n",
    "        \"LAI_s2_Q3_2021\",\n",
    "        \"SAVI_s2_Q3_2021\",\n",
    "        \"MSAVI_s2_Q3_2021\",\n",
    "        \"MNDWI_s2_Q3_2021\",\n",
    "        \"blue_s2_Q4_2021\",\n",
    "        \"green_s2_Q4_2021\",\n",
    "        \"red_s2_Q4_2021\",\n",
    "        \"nir_s2_Q4_2021\",\n",
    "        \"swir_1_s2_Q4_2021\",\n",
    "        \"swir_2_s2_Q4_2021\",\n",
    "        \"red_edge_1_s2_Q4_2021\",\n",
    "        \"red_edge_2_s2_Q4_2021\",\n",
    "        \"red_edge_3_s2_Q4_2021\",\n",
    "        \"NDVI_s2_Q4_2021\",\n",
    "        \"LAI_s2_Q4_2021\",\n",
    "        \"SAVI_s2_Q4_2021\",\n",
    "        \"MSAVI_s2_Q4_2021\",\n",
    "        \"MNDWI_s2_Q4_2021\",\n",
    "        \"blue_s2_Q1_2022\",\n",
    "        \"green_s2_Q1_2022\",\n",
    "        \"red_s2_Q1_2022\",\n",
    "        \"nir_s2_Q1_2022\",\n",
    "        \"swir_1_s2_Q1_2022\",\n",
    "        \"swir_2_s2_Q1_2022\",\n",
    "        \"red_edge_1_s2_Q1_2022\",\n",
    "        \"red_edge_2_s2_Q1_2022\",\n",
    "        \"red_edge_3_s2_Q1_2022\",\n",
    "        \"NDVI_s2_Q1_2022\",\n",
    "        \"LAI_s2_Q1_2022\",\n",
    "        \"SAVI_s2_Q1_2022\",\n",
    "        \"MSAVI_s2_Q1_2022\",\n",
    "        \"MNDWI_s2_Q1_2022\",\n",
    "        \"blue_s2_annual_2021\",\n",
    "        \"green_s2_annual_2021\",\n",
    "        \"red_s2_annual_2021\",\n",
    "        \"nir_s2_annual_2021\",\n",
    "        \"swir_1_s2_annual_2021\",\n",
    "        \"swir_2_s2_annual_2021\",\n",
    "        \"red_edge_1_s2_annual_2021\",\n",
    "        \"red_edge_2_s2_annual_2021\",\n",
    "        \"red_edge_3_s2_annual_2021\",\n",
    "        \"smad_s2_annual_2021\",\n",
    "        \"emad_s2_annual_2021\",\n",
    "        \"bcmad_s2_annual_2021\",\n",
    "        \"NDVI_s2_annual_2021\",\n",
    "        \"LAI_s2_annual_2021\",\n",
    "        \"SAVI_s2_annual_2021\",\n",
    "        \"MSAVI_s2_annual_2021\",\n",
    "        \"MNDWI_s2_annual_2021\",\n",
    "        \"blue_s2_semiannual_2021_01\",\n",
    "        \"green_s2_semiannual_2021_01\",\n",
    "        \"red_s2_semiannual_2021_01\",\n",
    "        \"nir_s2_semiannual_2021_01\",\n",
    "        \"swir_1_s2_semiannual_2021_01\",\n",
    "        \"swir_2_s2_semiannual_2021_01\",\n",
    "        \"red_edge_1_s2_semiannual_2021_01\",\n",
    "        \"red_edge_2_s2_semiannual_2021_01\",\n",
    "        \"red_edge_3_s2_semiannual_2021_01\",\n",
    "        \"smad_s2_semiannual_2021_01\",\n",
    "        \"emad_s2_semiannual_2021_01\",\n",
    "        \"bcmad_s2_semiannual_2021_01\",\n",
    "        \"NDVI_s2_semiannual_2021_01\",\n",
    "        \"LAI_s2_semiannual_2021_01\",\n",
    "        \"SAVI_s2_semiannual_2021_01\",\n",
    "        \"MSAVI_s2_semiannual_2021_01\",\n",
    "        \"MNDWI_s2_semiannual_2021_01\",\n",
    "        \"blue_s2_semiannual_2021_06\",\n",
    "        \"green_s2_semiannual_2021_06\",\n",
    "        \"red_s2_semiannual_2021_06\",\n",
    "        \"nir_s2_semiannual_2021_06\",\n",
    "        \"swir_1_s2_semiannual_2021_06\",\n",
    "        \"swir_2_s2_semiannual_2021_06\",\n",
    "        \"red_edge_1_s2_semiannual_2021_06\",\n",
    "        \"red_edge_2_s2_semiannual_2021_06\",\n",
    "        \"red_edge_3_s2_semiannual_2021_06\",\n",
    "        \"smad_s2_semiannual_2021_06\",\n",
    "        \"emad_s2_semiannual_2021_06\",\n",
    "        \"bcmad_s2_semiannual_2021_06\",\n",
    "        \"NDVI_s2_semiannual_2021_06\",\n",
    "        \"LAI_s2_semiannual_2021_06\",\n",
    "        \"SAVI_s2_semiannual_2021_06\",\n",
    "        \"MSAVI_s2_semiannual_2021_06\",\n",
    "        \"MNDWI_s2_semiannual_2021_06\",\n",
    "        \"vv_s1_xrgm_Q3_2021\",\n",
    "        \"vh_s1_xrgm_Q3_2021\",\n",
    "        \"vv_s1_xrgm_Q4_2021\",\n",
    "        \"vh_s1_xrgm_Q4_2021\",\n",
    "        \"bs_median_Q3_2021\",\n",
    "        \"pv_median_Q3_2021\",\n",
    "        \"npv_median_Q3_2021\",\n",
    "        \"ue_median_Q3_2021\",\n",
    "        \"bs_median_Q4_2021\",\n",
    "        \"pv_median_Q4_2021\",\n",
    "        \"npv_median_Q4_2021\",\n",
    "        \"ue_median_Q4_2021\",\n",
    "        \"bs_median_Q1_2022\",\n",
    "        \"pv_median_Q1_2022\",\n",
    "        \"npv_median_Q1_2022\",\n",
    "        \"ue_median_Q1_2022\",\n",
    "        \"rainfall_mean_Q3_2021\",\n",
    "        \"rainfall_mean_Q4_2021\",\n",
    "        \"rainfall_mean_Q1_2022\",\n",
    "        \"slope\",\n",
    "    ]\n",
    "\n",
    "# Set flag for removing correlated features (applied to training set to identify, then removed from test set for evaluation)\n",
    "remove_correlated_features = True\n",
    "removal_threshold = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6f785-6bb6-4726-bd8f-f241323e867b",
   "metadata": {},
   "source": [
    "## Convert model input into sklearn format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e54bf2-2fb4-4e74-b675-e7797788b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into a Pandas DataFrame, then split into features and labels\n",
    "model_input_df = pd.DataFrame(model_input, columns=column_names)\n",
    "X = model_input_df.drop(label_col, axis=1)[columns_to_use].values\n",
    "y = model_input_df[[label_col]].values.ravel()\n",
    "\n",
    "# Investigate value counts for each class\n",
    "model_input_df[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be622f-c7ba-4e2a-a8f0-22264ab2b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b5b25-65b3-4db9-ac44-fa67e730aaaf",
   "metadata": {},
   "source": [
    "## Fit, tune and evaluate multiple models using nested cross-validation\n",
    "\n",
    "This step allows us to train and tune mutliple models on fixed subsets of our data.\n",
    "\n",
    "When performing cross validation, data is split into `n` folds. One fold is kept aside as test data, and the rest is used to train a model. This step is repeated until each fold has been used as a test set, having been trained on the other two. From each fold, we get an estimate of the performance, which can be averaged to understand expected performance of a model on unseen data.\n",
    "\n",
    "Nested cross-validation introduces an additional step. Each set of training data is split into `m` further folds, and one is kept aside as test data specifically for fitting hyperparameters. The best parameters identifed across the `m` folds are then passed to the performance estimation folds.\n",
    "\n",
    "These steps are shown in the image below, with the larger green folds showing the performance step with `n=3` folds, and the hyperparameter tuning step with `m=4` folds.\n",
    "\n",
    "<img align=\"center\" src=\"../../../Supplementary_data/Scalable_machine_learning/nested_CV.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5745827-ca55-476b-bd89-eaa0acb20a8c",
   "metadata": {},
   "source": [
    "### Get number of cpus available for nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8f53f-12c6-49e3-b3b9-b73d79eb8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a6282-7190-465a-9aee-b790f0326cf7",
   "metadata": {},
   "source": [
    "### Construct the models and their parameter grids for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e700b97-fe74-406e-a3ed-a5895b956203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store models\n",
    "models = []\n",
    "\n",
    "\n",
    "# Random forest grid and model\n",
    "model_name = \"RandomForest\"\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"model__class_weight\": [\"balanced\", None],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"model__n_estimators\": [200, 300, 400],\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "models.append((model_name, RandomForestClassifier(n_jobs=1), rf_param_grid))\n",
    "\n",
    "# Ada Boost grid and model\n",
    "model_name = \"AdaBoost\"\n",
    "\n",
    "ab_param_grid = {\n",
    "    \"model__base_estimator\": [DecisionTreeClassifier(max_depth=i) for i in [1, 3, 10]],\n",
    "    \"model__n_estimators\": [10, 100, 1000],\n",
    "    \"model__learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "models.append((model_name, AdaBoostClassifier(), ab_param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106a443-051c-4980-b634-7539ead47aed",
   "metadata": {},
   "source": [
    "### Create pipeline elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32c6b5-64ac-4a49-a53f-ee3a939afefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer for removing correlated variables\n",
    "\n",
    "\n",
    "class DropCorrelatedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, removal_threshold=0.9, to_drop=None):\n",
    "        self.cols = cols\n",
    "        self.removal_threshold = removal_threshold\n",
    "        self.to_drop = to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_ = pd.DataFrame(X_, columns=self.cols)  # X_[self.cols]\n",
    "\n",
    "        correlation_matrix = X_.corr().abs()\n",
    "        upper_tri = correlation_matrix.where(\n",
    "            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        to_drop = [\n",
    "            column\n",
    "            for column in upper_tri.columns\n",
    "            if any(upper_tri[column] > self.removal_threshold)\n",
    "        ]\n",
    "\n",
    "        self.to_drop = to_drop\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_ = pd.DataFrame(X_, columns=self.cols)\n",
    "\n",
    "        X_ = X_.drop(self.to_drop, axis=1).values\n",
    "\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba41e89-3817-4d1e-ab0d-1ec66b44d9b5",
   "metadata": {},
   "source": [
    "### Perform nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a8a8f-eb85-4086-8595-5dd89fa1d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store outputs\n",
    "results = {}\n",
    "outer_cv_test_pairs = {}\n",
    "pipelines = {}\n",
    "\n",
    "# Only run a single trial for each algorithm, so set a single seed to use for selecting folds\n",
    "cv_seed = 13\n",
    "model_seed = 32\n",
    "\n",
    "# Set number of splits to do\n",
    "inner_cv_splits = 3\n",
    "outer_cv_splits = 3\n",
    "\n",
    "# Number of jobs to pass to the inner cross validation loop\n",
    "n_jobs_outer = 3\n",
    "n_jobs_inner = ncpus - n_jobs_outer\n",
    "\n",
    "for name, model, p_grid in models:\n",
    "    print(f\"Running {name}\")\n",
    "\n",
    "    # Create the pipeline method to leverage\n",
    "    if remove_correlated_features:\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\n",
    "                    \"drop_corr_features\",\n",
    "                    DropCorrelatedFeatures(\n",
    "                        columns_to_use, removal_threshold=removal_threshold\n",
    "                    ),\n",
    "                ),\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    pipelines[name] = pipeline\n",
    "\n",
    "    # Create the outer_cv for each model so that the same data is fitted\n",
    "    outer_cv = StratifiedKFold(\n",
    "        n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed\n",
    "    )\n",
    "\n",
    "    # Create dictionary to store testing arrays for each model\n",
    "    model_cv_test_pairs = {}\n",
    "    model_best_estimators = {}\n",
    "\n",
    "    # Loop over the outer split\n",
    "    for outer_split_number, (train_index, test_index) in enumerate(\n",
    "        outer_cv.split(X, y)\n",
    "    ):\n",
    "        print(f\"running Outer Split {outer_split_number}\")\n",
    "\n",
    "        X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Create inner cv for each outer cv\n",
    "        inner_cv = StratifiedKFold(\n",
    "            n_splits=inner_cv_splits, shuffle=True, random_state=cv_seed\n",
    "        )\n",
    "\n",
    "        # Create grid search\n",
    "        clf = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=p_grid,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=inner_cv,\n",
    "            n_jobs=n_jobs_inner,\n",
    "        )\n",
    "\n",
    "        print(\"    fitting inner CV loop\")\n",
    "        # Fit to training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate prediction\n",
    "        best_model = clf.best_estimator_\n",
    "        print(\"performing prediction\")\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        test_f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "        # Store the results\n",
    "        model_best_estimators[f\"split_{outer_split_number}\"] = {\n",
    "            \"best_estimator\": clf.best_estimator_,\n",
    "            \"f1_macro_score\": test_f1_macro,\n",
    "        }\n",
    "\n",
    "        # Store the true and predicted arrays\n",
    "        model_cv_test_pairs[f\"split_{outer_split_number}\"] = (y_test, y_pred)\n",
    "\n",
    "    # Capture results out\n",
    "    outer_cv_test_pairs[name] = model_cv_test_pairs\n",
    "    results[name] = model_best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b6857-28c3-4b56-b559-2a63d0f7ce51",
   "metadata": {},
   "source": [
    "### Display average scores over outer folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6bf96-d836-4d6e-b9dd-f99acdb7bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in results.keys():\n",
    "    scores = np.array(\n",
    "        [results[key][split][\"f1_macro_score\"] for split in results[key].keys()]\n",
    "    )\n",
    "    mean = scores.mean()\n",
    "    std = scores.std()\n",
    "\n",
    "    print(f\"Average F1 Macro for {key} = {mean:.2f} with std. dev. of {std:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b90f2d-5181-4aa7-bd88-18d27b68af23",
   "metadata": {},
   "source": [
    "## Investigate results\n",
    "\n",
    "One of the most useful ways to get insight into the performance of machine learning classifiers is to view the confusion matrix, which counts the number of points in each predicted class as a function of the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ade559-cf68-40cb-a3d3-363d28c70dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model, p_grid in models:\n",
    "\n",
    "    # clf_list = results[f\"{name}\"]\n",
    "    val_file = f\"results/{experiment_name}_{name}_confusionmat_values.png\"\n",
    "    norm_file = f\"results/{experiment_name}_{name}_confusionmat_normalised.png\"\n",
    "\n",
    "    val_fig, val_ax = plt.subplots(1, outer_cv_splits, figsize=(8 * outer_cv_splits, 7))\n",
    "    val_fig.suptitle(f\"{name} Confusion Matrix (Value)\")\n",
    "    norm_fig, norm_ax = plt.subplots(\n",
    "        1, outer_cv_splits, figsize=(8 * outer_cv_splits, 7)\n",
    "    )\n",
    "    norm_fig.suptitle(f\"{name} Confusion Matrix (Row Normalised)\", fontsize=16)\n",
    "\n",
    "    for i in range(outer_cv_splits):\n",
    "        y_true = outer_cv_test_pairs[name][f\"split_{i}\"][0]\n",
    "        y_pred = outer_cv_test_pairs[name][f\"split_{i}\"][1]\n",
    "        # Plot unormalised confusion matrix\n",
    "        val_ax[i].set_title(f\"Fold {i}\")\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            normalize=None,\n",
    "            ax=val_ax[i],\n",
    "            colorbar=False,\n",
    "            display_labels=list(labels_dict.keys()),\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "\n",
    "        val_fig.savefig(val_file, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "\n",
    "        # Plot normalised confusion matrix\n",
    "        norm_ax[i].set_title(f\"Fold {i}\")\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            normalize=\"true\",\n",
    "            ax=norm_ax[i],\n",
    "            colorbar=False,\n",
    "            display_labels=list(labels_dict.keys()),\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "\n",
    "        norm_fig.savefig(norm_file, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b1367-7056-498c-8579-939eac90a01a",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with outer folds\n",
    "\n",
    "After running nested cross-validation, we have selected the best model and understood the performance we can expect to see on new data.\n",
    "\n",
    "Once the model is decided, we can run hyperparameter tuning using the outer fold only, allowing us to tune the model with additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d2f7e-14f2-44f1-92bb-83611d91fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc671b6-34e1-467a-9af1-aa484b6ebeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimated params for RF model\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=outer_cv_splits, shuffle=True, random_state=cv_seed)\n",
    "\n",
    "metric = \"f1_macro\"\n",
    "name, model, p_grid = models[0]  # Use random forest\n",
    "\n",
    "# instatiate a gridsearchCV using outer cross-validation folds\n",
    "clf = GridSearchCV(\n",
    "    pipelines[name],\n",
    "    p_grid,\n",
    "    scoring=metric,\n",
    "    verbose=1,\n",
    "    cv=outer_cv.split(X, y),\n",
    "    n_jobs=ncpus,\n",
    ")\n",
    "\n",
    "# Fit the gridsearch on outer cross-validation folds\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print(\"\\n\")\n",
    "print(\"The \" + metric + \" score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623accb-885c-425c-8192-ea0f103d2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_correlated_features:\n",
    "    removed_cols = clf.best_estimator_[\"drop_corr_features\"].to_drop\n",
    "    remaining_cols = [col for col in columns_to_use if col not in removed_cols]\n",
    "else:\n",
    "    remaining_cols = columns_to_use\n",
    "\n",
    "remaining_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cac00e-5fd8-469c-af65-1a6ec9e0b043",
   "metadata": {},
   "source": [
    "## Final model fit\n",
    "\n",
    "The cross-validation steps have allowed us to pick the best performing model on unseen data and further tune that model. The final step is to fit the model to all of the data, using the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9731e9-4532-40c4-a731-e78282affd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data and fit new model\n",
    "X_transformed = clf.best_estimator_[\"drop_corr_features\"].transform(X)\n",
    "\n",
    "new_model = clf.best_estimator_[\"model\"]\n",
    "new_model.fit(X_transformed, y)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "\n",
    "# Export the final model for use in following notebooks\n",
    "dump(new_model, f\"results/{experiment_name}_{name}.joblib\")\n",
    "\n",
    "# Export the columns to use in the final model\n",
    "with open(\n",
    "    f\"results/{experiment_name}_{name}_features.json\", \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump({\"features\": remaining_cols}, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
