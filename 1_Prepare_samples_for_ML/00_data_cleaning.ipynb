{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeca6764-6194-4fc3-bafd-e93d05f277a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766c94a-5a17-44f1-a08f-dbf8ca381b08",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to clean unmodifed or modifed csv files collected from ECAAS ODK. \n",
    "\n",
    "The notebook contains the following steps:\n",
    "1. Read in and display an ECAAS ODK csv\n",
    "2. General Cleaning\n",
    "3. Clean crop type attributes\n",
    "5. Construct dictionary for renaming crops\n",
    "6. Create geometries (working with ODK collected coordinates or working with manually edited coordinates)\n",
    "9. Export data to geojson and Shapefile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863da5c-effc-47b3-b2c3-daa8976bf018",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097775f8-b2d5-4e85-a6dc-647e785b0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from create_geometries import (\n",
    "    create_geometries_from_manual_edit,\n",
    "    create_geometries_from_ODK,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79609fa-b793-46d9-b742-71bfca647496",
   "metadata": {},
   "source": [
    "## Read in ECAAS ODK csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55be62d-e8c1-4bd6-872f-c2fcef75e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = \"data/Cleaned_ECAAS_ODK_Datasheet.csv\"\n",
    "samples_df = pd.read_csv(samples_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a2dfe-fa79-4109-8c7a-70e43bb2373a",
   "metadata": {},
   "source": [
    "##  Initial inspection\n",
    "\n",
    "To understand the kind of data available, it is useful to view the first 5 rows, and then view more detailed information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884a94c-aa5b-4b02-9a22-0d292c49e86d",
   "metadata": {},
   "source": [
    "### First five rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f059f-bdd4-4394-9a77-4ff4b87a2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403809e6-c4f5-49fc-9127-186380d1e785",
   "metadata": {},
   "source": [
    "### Display column names, non-null count and dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd45cee-8440-4f73-b73f-9810f32dd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa953fab-bfb9-47ce-8199-9345dd7321da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General Cleaning\n",
    "\n",
    "* Remove unnecessary prefixes from columns to improve readability\n",
    "* Convert missing values from \" \" (single space) to None\n",
    "* Convert start and end times from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6df7b-49e0-4b19-82ea-4b067e179433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove prefixes from columns to improve readability\n",
    "updated_columns = samples_df.columns.str.replace(\"data-\", \"\")\n",
    "updated_columns = updated_columns.str.replace(\"consent_given-\", \"\")\n",
    "updated_columns = updated_columns.str.replace(\"field_planted-\", \"\")\n",
    "\n",
    "samples_df.columns = updated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cffca-1988-4764-beea-560b8b6ba30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert from missing values being \" \" to None\n",
    "samples_df = samples_df.replace({\" \": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb044b0-1c30-48e2-8462-97fb0ea74247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to date strings. Must be string format for shapefile\n",
    "samples_df[\"start\"] = pd.to_datetime(samples_df[\"start\"], dayfirst=True).dt.strftime(\"%Y-%m-%d\")\n",
    "samples_df[\"end\"] = pd.to_datetime(samples_df[\"end\"], dayfirst=True).dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1fed2-7aaa-408e-b629-b58aabe17d90",
   "metadata": {},
   "source": [
    "### Display column names, non-null count and dtype after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7c88e-f5a4-405f-9142-8d318605eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ac93b-2dcc-4b47-b84e-6285aca6a96a",
   "metadata": {},
   "source": [
    "## Clean crop type attributes\n",
    "\n",
    "The crop name of the primary crop is stored in the `primary_crop` field if selected from the toolkit list, and the `primary_crop_other` field otherwise. The same is true of the `secondary_crop` and `secondary_crop_other` columns. The following steps are used to produce single unified `primary_crop` and `secondary_crop` fields:\n",
    "\n",
    "For each pair, the following steps are run:\n",
    "\n",
    "1. Identify all rows containing a valid entry in the `crop_other` field.\n",
    "2. Convert these entries to lower case to match entries in the `crop` field.\n",
    "3. Copy valid entries from the `crop_other` field to the `crop` field.\n",
    "4. Drop the `crop_other` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cf035-41f5-49c0-aebd-ac3d81870a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"primary\", \"secondary\"]\n",
    "\n",
    "for prefix in prefixes:\n",
    "    \n",
    "    other_crop_condition = samples_df.loc[:, f\"{prefix}_crop_other\"].notna()\n",
    "    \n",
    "    samples_df.loc[other_crop_condition, f\"{prefix}_crop\"] = samples_df.loc[other_crop_condition, f\"{prefix}_crop_other\"].str.lower()\n",
    "    \n",
    "    samples_df = samples_df.drop(f\"{prefix}_crop_other\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4e361-7455-49a9-9f2a-a6ede20c91bf",
   "metadata": {},
   "source": [
    "### View unique crops and value counts\n",
    "\n",
    "Now that all crops have been collated into a single column and format, running `value_counts` will display the unique spellings and counts. After viewing this, you will need to compile a dictionary to coerce varied spellings for the same crop into a single spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08729479-517f-4fc3-ad7f-9da8c801ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts for all instances in each crop column\n",
    "for prefix in prefixes:\n",
    "    \n",
    "    print(samples_df[[f\"{prefix}_crop\"]].value_counts(\n",
    "        dropna=False\n",
    "    ).sort_index(ascending=True))\n",
    "\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa562537-ed4f-4923-a0ca-94b1d70f2782",
   "metadata": {},
   "source": [
    "### Construct dictionary for renaming crops\n",
    "\n",
    "Based on the values in the list above, we compiled the following dictionary, then applied it to the `primary_crop` and `secondary_crop` fields. This step will rename appearances of the dictionary key to the corresponding value. For example \"bananas\" will become \"banana\".\n",
    "\n",
    "After running the next two cells, review the cleaned crop list to check if any additions need to be made to the dictionary. Repeat as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd7e90-f8d7-4b0f-9939-c8485212d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to clean any remaining mismatched data\n",
    "crop_dictionary = {\n",
    "    \"bananas\": \"banana\",\n",
    "    \"groundnuts\": \"groundnut\",\n",
    "    \"macadamia nuts\": \"macadamia\",\n",
    "    \"macadamia nut\": \"macadamia\",\n",
    "    \"ochra vegetables\": \"ochra\",\n",
    "    \"okra\": \"ochra\",\n",
    "    \"soyabean\": \"soyabean\",\n",
    "    \"sweet potatoes\": \"sweet potato\",\n",
    "    \"water melon\": \"watermelon\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748e3fd-9f4e-439f-a336-707eed73d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the dictionary to the crop columns\n",
    "for prefix in prefixes:\n",
    "    samples_df.loc[:, f\"{prefix}_crop\"] = samples_df.loc[:, f\"{prefix}_crop\"].replace(\n",
    "        crop_dictionary\n",
    "    )\n",
    "\n",
    "    print(samples_df[f\"{prefix}_crop\"].value_counts())\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064f3a6-d340-4a6c-8e2e-32ee767cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = samples_df['primary_crop'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "counts[counts >= 10].plot(kind='bar', ax=axes[0])\n",
    "counts[counts < 10].plot(kind='bar', ax=axes[1])\n",
    "axes[0].set_title(\"Crops with more than 10 samples\")\n",
    "axes[1].set_title(\"Crops with fewer than 10 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30efc0-d25e-4847-b381-4ebbf5bd71e7",
   "metadata": {},
   "source": [
    "## Create geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a6b3c-f926-439d-9f97-ac833baf8103",
   "metadata": {
    "tags": []
   },
   "source": [
    "This section converts geometries collected by the ECAAS ODK toolkit from strings to Shapely geomerties for use with the Open Data Cube. It can process:\n",
    "* A csv produced from the ECAAS ODK toolkit\n",
    "* A modified version of the csv from the ECAAS ODK toolkit\n",
    "\n",
    "---\n",
    "\n",
    "### Working with ODK collected coordinates\n",
    "\n",
    "The following logic is used to extract point and polygon geometries from the ODK output:\n",
    "\n",
    "- If `access_consent == \"yes\"`, then row should have field boundary and field center\n",
    "    - centre point is given as a \"lat,lon\" point\n",
    "    - boundary is given as \"lat lon alt acc; lat lon alt acc;\" for each point collected along the boundary, separated by semi-colons\n",
    "- If `access_consent == \"no\"`, then row should only have field outside corner (no boundary)\n",
    "    - outer corner point is given as a \"lat,lon\"\n",
    "    \n",
    "For points, the method creates a new `point_location` location field, with values of either \"center\" or \"outside corner\".\n",
    "\n",
    "---\n",
    "\n",
    "### Working with manually edited coordinates\n",
    "\n",
    "Specifically, after collecting points using the ECAAS ODK toolkit, an analyst may wish to manually update the points, for example, moving them from the road to within the field. If they have provided these as a new `lat,lon` pair in a new column of the csv, these points may be used instead of the original ODK points. \n",
    "\n",
    "Different cleaning routines will be run depending on the type of file supplied. The user may dictate which process to use by specifying the variable `cleaned_geom_column` as:\n",
    "* `None` for an unmodified file (uses ODK collected coordinates)\n",
    "* `\"column_name\"` for a modified file -- where `column_name` contains the modified lat,lon values in the csv (uses modified coordinates).\n",
    "\n",
    "If using modified coordinates, the `point_location` field will be given as \"manual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fdd5b-fdff-4d39-9498-80d233e5a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with original ODK toolkit output\n",
    "# cleaned_geom_column = None\n",
    "\n",
    "# use with modified ODK toolkit output. The column listed below must contain data in lat,lon format (i.e. -14.4,28.0)\n",
    "cleaned_geom_column = \"Cleaned_Coordinates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949f3e8-a784-408c-bfa1-275dfb9a28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cleaned_geom_column is None:\n",
    "    # Return cleaned samples from ECAAS ODK format, containing mix of points and polygons\n",
    "    cleaned_samples_df = create_geometries_from_ODK(samples_df)\n",
    "\n",
    "    # Create two geodataframes, one with point geometry and one with polygon geometry\n",
    "    points_gdf = cleaned_samples_df.drop([\"field_boundary_polygon\"], axis=\"columns\").copy()\n",
    "    polygons_gdf = cleaned_samples_df.set_geometry(\"field_boundary_polygon\", drop=True).copy()\n",
    "    polygons_gdf = polygons_gdf.loc[\n",
    "        ~cleaned_samples_df[\"field_boundary_polygon\"].isna(), :\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    # Retun geodataframe containing point geometry, extracted from cleaned_geom_column\n",
    "    points_gdf = create_geometries_from_manual_edit(samples_df, cleaned_geom_column)\n",
    "    polygons_gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983aa752-3949-44c8-8c3a-fbcf038b47fc",
   "metadata": {},
   "source": [
    "## Export data to geojson and Shapefile\n",
    "\n",
    "For the final output, we will export all points and polygons, as well as the cleaned single crop file. The cleaned single crop file will be used for the remainder of the machine learning training.\n",
    "\n",
    "We define a dictionary for the column names to be shortened to when writing to the Shapefile format (10 character limit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f5de6-277d-4c1c-ba21-be8e212d24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with columns of interest and corresponding 10-character names\n",
    "col_rename_dict = {\n",
    "    \"start\": \"start\",\n",
    "    \"end\": \"end\",\n",
    "    \"field_fallow\": \"fallow\",\n",
    "    \"primary_crop_type\": \"pri_type\",\n",
    "    \"primary_crop\": \"pri_crop\",\n",
    "    \"crop_development\": \"crop_dev\",\n",
    "    \"multiple_crops\": \"multi_crop\",\n",
    "    \"multiple_crops_percentage\": \"multi_per\",\n",
    "    \"secondary_crop\": \"sec_crop\",\n",
    "    \"geometry\": \"geometry\",\n",
    "}\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "    \n",
    "# Export polygons\n",
    "if polygons_gdf is not None:\n",
    "\n",
    "    polygons_gdf[col_rename_dict.keys()].to_file(\"results/cleaned_polygons.geojson\")\n",
    "    polygons_gdf[col_rename_dict.keys()].rename(columns=col_rename_dict).to_file(\n",
    "        \"results/cleaned_polygons.shp\"\n",
    "    )\n",
    "\n",
    "# Add additional column to specify the point location\n",
    "col_rename_dict[\"point_location\"] = \"point_loc\"\n",
    "\n",
    "# Export points\n",
    "points_gdf[col_rename_dict.keys()].to_file(\"results/cleaned_points.geojson\")\n",
    "points_gdf[col_rename_dict.keys()].rename(columns=col_rename_dict).to_file(\n",
    "    \"results/cleaned_points.shp\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
